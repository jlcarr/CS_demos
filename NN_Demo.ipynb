{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks\n",
    "https://en.wikipedia.org/wiki/Artificial_neural_network  \n",
    "Artificial neural networks (ANNs) are a popular machine learning model inspired by the biological neural networks in our brains. As with other machine learning models, by fitting the models with input data with labeled outputs the neural network can \"learn\" the function that maps the input to the output, and extrapolate to new examples.  \n",
    "\n",
    "This demo focus on visualizing what a neural network looks like, how it operates, and give examples on how to create and train them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for array calculations\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to define and train neural network models\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to visualize graphs (in this case: neural networks)\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anatomy of Simple Neural Networks\n",
    "https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics)  \n",
    "https://en.wikipedia.org/wiki/Tensor  \n",
    "https://en.wikipedia.org/wiki/Vectorization_(mathematics)  \n",
    "As with most machine learning models, data needs to be **vectorized**, i.e. turned into a array of numbers, so that the computer can work with it.  \n",
    "Sometimes we might change the shape of the vector to be a multidimensional array instead instead of a simple array. We may then think of the vector as a **tensor**.  \n",
    "For the purposes of this demo we will only be looking at data in the form of simple vectors as 1D arrays.  \n",
    "\n",
    "A neural network takes an **input vector**, and transforms if via a series of linear and non-linear transformations into an **output vector**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the NN's layers \n",
    "layers = [] # Input layer is simply defined by an input shape\n",
    "layers.append(keras.layers.Dense(8, activation='sigmoid', input_shape=(2,))) # 1st Hidden layer\n",
    "layers.append(keras.layers.Dense(1, activation='sigmoid')) # Output layer\n",
    "\n",
    "# Join the layers together\n",
    "model = keras.Sequential(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph object from the NN\n",
    "NN_graph = nx.DiGraph()\n",
    "for i_layer,layer in enumerate(model.layers):\n",
    "    for (i_input,i_output),weight in np.ndenumerate(layer.get_weights()[0]):\n",
    "        input_label = f\"{i_layer}:{i_input}\"\n",
    "        output_label = f\"{i_layer+1}:{i_output}\"\n",
    "        if i_input==0:\n",
    "            bias = layer.get_weights()[1][i_output]\n",
    "            NN_graph.add_node(output_label, bias=bias)\n",
    "        NN_graph.add_edge(input_label, output_label, weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layout and display the graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the model weights and biases as tensors\n",
    "for i,layer in enumerate(model.layers):\n",
    "    weights = layer.get_weights()[0]\n",
    "    biases = layer.get_weights()[1]\n",
    "    print(\"Layer {} Weights:\".format(i))\n",
    "    print(weights)\n",
    "    print(\"Layer {} Biases:\".format(i))\n",
    "    print(weights)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "https://en.wikipedia.org/wiki/Universal_approximation_theorem  \n",
    "https://en.wikipedia.org/wiki/Machine_learning  \n",
    "https://en.wikipedia.org/wiki/Mathematical_optimization  \n",
    "The training, or learning, of an machine learning model is accomplished by performing an optimization over the model's parameters to minimize the error between the model's predicted values, and the true labeled values from the training data.  \n",
    "\n",
    "Mathematically we can write training as solving the following problem:  \n",
    "$$ \\min_{\\vec{w}} \\sum_{i} C \\left (  f \\left ( \\vec{X}_{i}, \\vec{w} \\right ),  \\vec{y}_{i} \\right ) $$\n",
    "Where:  \n",
    "$\\vec{w}$ is the vector of parameters (weights) of the model.  \n",
    "$\\vec{X}_{i}$ is the input vector of the $i$th datapoint in the training set.  \n",
    "$\\vec{y}_{i}$ is the output vector (label) of the $i$th datapoint in the training set.  \n",
    "$f$ Is the function evaluated by the model.  \n",
    "$C$ is the loss (cost) function.  \n",
    "\n",
    "As one can see, this is the classic formulation of an optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
