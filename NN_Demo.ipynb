{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks\n",
    "https://en.wikipedia.org/wiki/Artificial_neural_network  \n",
    "Artificial neural networks (ANNs) are a popular machine learning model inspired by the biological neural networks in our brains. As with other machine learning models, by fitting the models with input data with labeled outputs the neural network can \"learn\" the function that maps the input to the output, and extrapolate to new examples.  \n",
    "\n",
    "This demo focus on visualizing what a neural network looks like, how it operates, and give examples on how to create and train them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for array calculations\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to define and train neural network models\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to visualize graphs (in this case: neural networks)\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classic Python visualization library\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "https://en.wikipedia.org/wiki/Vector_(mathematics_and_physics)  \n",
    "https://en.wikipedia.org/wiki/Tensor  \n",
    "https://en.wikipedia.org/wiki/Vectorization_(mathematics)  \n",
    "As with most machine learning models, data needs to be **vectorized**, i.e. turned into a array of numbers, so that the computer can work with it.  \n",
    "Sometimes we might change the shape of the vector to be a multidimensional array instead instead of a simple array. We may then think of the vector as a **tensor**.  \n",
    "For the purposes of this demo we will only be looking at data in the form of simple vectors as 1D arrays.  \n",
    "\n",
    "The power of a neural network is its ability to learn any function mapping any input vector to any output vector.  \n",
    "In this demo we will demonstrate this property by making a binary function for the neural network to learn. Considering the fact that making binary functions is what the transistors in our computer do, we can see the power of a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit this!\n",
    "n_inputs = 2\n",
    "my_function = lambda x: x[0] ^ x[1]\n",
    "\n",
    "# Turn the function into a truth table for training data\n",
    "X = []\n",
    "y = []\n",
    "for bin_input in itertools.product([0,1], repeat=n_inputs):\n",
    "    print(f\"Input: {bin_input} Output: {my_function(bin_input)}\")\n",
    "    X.append(bin_input)\n",
    "    y.append(my_function(bin_input))\n",
    "X = np.array(X, dtype=np.float64)\n",
    "y = np.array(y, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Description of Simple Neural Networks\n",
    "### Mathematical Description of a Neural Network\n",
    "A neural network takes an **input vector**, and transforms if via a series of linear and non-linear transformations into an **output vector**.  \n",
    "Mathematically this can be described by the following recurrence relation:  \n",
    "$$ \\vec{x}_{i+1} = g \\left ( W_{i+1} \\vec{x}_{i} + \\vec{b}_{i+1} \\right )$$\n",
    "Where:  \n",
    "$\\vec{x}_{i}$ is the vector after $i$ transformations.  \n",
    "$N$ is the number of layers in the neural network.  \n",
    "$\\vec{x}_{0}$ is the input vector.  \n",
    "$\\vec{x}_{N}$ is the output vector.  \n",
    "$W_{i}$ is the weights matrix of the connections between layer $i-1$ and $i$.  \n",
    "$\\vec{b}_{i}$ is the bias vector of layer $i$.  \n",
    "$g$ is the activation function applied elementwise to a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the NN's layers \n",
    "layers = [] # Input layer is simply defined by an input shape\n",
    "layers.append(keras.layers.Dense(8, activation='sigmoid', input_shape=(2,))) # 1st Hidden layer\n",
    "layers.append(keras.layers.Dense(1, activation='sigmoid')) # Output layer\n",
    "\n",
    "# Join the layers together\n",
    "model = keras.Sequential(layers)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the model weights and biases as tensors\n",
    "for i,layer in enumerate(model.layers):\n",
    "    weights = layer.get_weights()[0]\n",
    "    biases = layer.get_weights()[1]\n",
    "    print(\"Layer {} Weights:\".format(i))\n",
    "    print(weights)\n",
    "    print(\"Layer {} Biases:\".format(i))\n",
    "    print(biases)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks as Graphs\n",
    "https://en.wikipedia.org/wiki/Directed_graph  \n",
    "https://en.wikipedia.org/wiki/Directed_acyclic_graph  \n",
    "Often we like to look at the connections a directed graph, where the nodes are the neurons and the edges are the connections between the neurons.  \n",
    "The graph is directed and acyclic.  \n",
    "We can take the representation a step further by thinking of the neural network as a weighted graph, in which the weights of the edges are the connection weights, and the weights of the nodes are the biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph object from the NN\n",
    "NN_graph = nx.DiGraph()\n",
    "for i_layer,layer in enumerate(model.layers):\n",
    "    for (i_input,i_output),weight in np.ndenumerate(layer.get_weights()[0]):\n",
    "        input_label = f\"{i_layer}:{i_input}\"\n",
    "        output_label = f\"{i_layer+1}:{i_output}\"\n",
    "        if i_input==0:\n",
    "            bias = layer.get_weights()[1][i_output]\n",
    "            NN_graph.add_node(output_label, bias=bias)\n",
    "        NN_graph.add_edge(input_label, output_label, weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layout and display the graph\n",
    "pos = nx.nx_agraph.graphviz_layout(NN_graph, prog='dot', args=\"-Grankdir=LR\")\n",
    "nx.draw_networkx(NN_graph, pos=pos, node_size=2000, node_color='lightskyblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "### Problem Definition\n",
    "https://en.wikipedia.org/wiki/Universal_approximation_theorem  \n",
    "https://en.wikipedia.org/wiki/Machine_learning  \n",
    "The training, or learning, of an machine learning model is accomplished by performing an optimization over the model's parameters to minimize the error between the model's predicted values, and the true labeled values from the training data.  \n",
    "\n",
    "Mathematically we can write training as solving the following problem:  \n",
    "$$ \\min_{\\vec{w}} \\sum_{i} C \\left (  f \\left ( \\vec{X}_{i}, \\vec{w} \\right ),  \\vec{y}_{i} \\right ) $$\n",
    "Where:  \n",
    "$\\vec{w}$ is the vector of parameters (weights) of the model.  \n",
    "$\\vec{X}_{i}$ is the input vector of the $i$th datapoint in the training set.  \n",
    "$\\vec{y}_{i}$ is the output vector (label) of the $i$th datapoint in the training set.  \n",
    "$f$ Is the function evaluated by the model.  \n",
    "$C$ is the loss (cost) function.  \n",
    "\n",
    "As one can see, this is the classic formulation of an optimization problem.  \n",
    "\n",
    "### Objective/Cost/Loss Functions\n",
    "https://en.wikipedia.org/wiki/Loss_function  \n",
    "In machine learning the terms \"objective function\", \"cost function\" and \"loss function\" all refer to the same concept: a measure of error of the model that we seek to minimize.  \n",
    "\n",
    "Let's look at a few different popular loss functions:  \n",
    "\n",
    "#### Absolute Error\n",
    "https://en.wikipedia.org/wiki/Approximation_error  \n",
    "The simplest loss function. Simple to compute, and intuitive to understand.  \n",
    "$$ C \\left ( \\vec{y}_{\\mathrm{pred}}, \\vec{y}_{\\mathrm{true}} \\right ) = \\left \\| \\vec{y}_{\\mathrm{pred}} - \\vec{y}_{\\mathrm{true}} \\right \\| _{1} = \\sum_i \\left | y_{\\mathrm{pred}, i} - y_{\\mathrm{true}, i} \\right | $$\n",
    "\n",
    "#### Quadratic (Least Squares)\n",
    "https://en.wikipedia.org/wiki/Least_squares  \n",
    "The most commonly known loss function. Simple to compute, and intuitive to understand in terms of distance in Euclidean space.  \n",
    "$$ C \\left ( \\vec{y}_{\\mathrm{pred}}, \\vec{y}_{\\mathrm{true}} \\right ) = \\left \\| \\vec{y}_{\\mathrm{pred}} - \\vec{y}_{\\mathrm{true}} \\right \\| ^{2} = \\sum_i \\left ( y_{\\mathrm{pred}, i} - y_{\\mathrm{true}, i} \\right ) ^{2} $$\n",
    "\n",
    "#### Cross-Entropy\n",
    "https://en.wikipedia.org/wiki/Cross_entropy  \n",
    "https://en.wikipedia.org/wiki/Information_theory  \n",
    "A very popular loss function for machine learning classification problems. Simple to compute, but a little more difficult to get intuition from information theory: the average number of binary questions needed to identify an element chosen from distribution $\\vec{y}_{\\mathrm{true}}$ when optimizing the binary questions for distribution $\\vec{y}_{\\mathrm{pred}}$.\n",
    "$$ C \\left ( \\vec{y}_{\\mathrm{pred}}, \\vec{y}_{\\mathrm{true}} \\right ) = - \\sum_{i} y_{\\mathrm{true}, i} \\log \\left ( y_{\\mathrm{pred}, i} \\right ) $$  \n",
    "\n",
    "Many other loss functions exist. Many are simply variants of the above.  \n",
    "\n",
    "### Optimization Techniques\n",
    "https://en.wikipedia.org/wiki/Mathematical_optimization  \n",
    "https://en.wikipedia.org/wiki/Optimization_problem  \n",
    "https://en.wikipedia.org/wiki/Stochastic_gradient_descent  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.1)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X)\n",
    "print(\"y actual:\")\n",
    "print(y)\n",
    "print(\"y predicted:\")\n",
    "print(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
