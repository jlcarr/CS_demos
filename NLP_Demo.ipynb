{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)\n",
    "Written language is composed of **strings of symbols** (alphabet), which are grouped into **words**, which are grouped into **sentences**.  \n",
    "Computers can parse strings of symbols into words, and even words into sentences, but extracting meaning from sentences, especially with all the **nuances** of expressions, emotions, context and such, can be extremely challenging. \n",
    "To simplify computation most machine-learning algorithms use **vector representations (embeddings)** of words and/or sentences.  \n",
    "Computers are very well suited to processing vectors: they are just ordered lists of numbers of fixed dimensionality after all, and **GPU**'s are hardware that are specifically designed to perform **massively parallel** computations on vectors.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK (Natural Language Toolkit)\n",
    "https://www.nltk.org/  \n",
    "NLTK is an NLP package that is designed to provide a suite of different NLP tools, models, algorithmns and corpora.  \n",
    "It is primarily meant for teaching and research.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy\n",
    "https://spacy.io/  \n",
    "Spacy is an NLP package that is designed to provide out-of-the-box NLP tools.  \n",
    "It is designed to have \"industrial strength\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_parser = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scikit-learn\n",
    "https://scikit-learn.org/  \n",
    "scikit-Learn is an ML package that is designed to provide a suite of classical ML tools.  \n",
    "It is designed to be very accessible and simple.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Parsing, Wrangling and Cleaning\n",
    "Raw text is a classic example of **unstructured data**. It's generally quite dirty, and contains a lot of unnecessary information.  \n",
    "The main preprocessing steps of a typical NLP pipelines are as follows:  \n",
    "1. **Tokenization**: Break the text up into words.\n",
    "2. **Lemmatization or Stemming**: Convert the words into their base forms.\n",
    "3. **Stop Word Filtering**: Remove words with little meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization  \n",
    "https://en.wikipedia.org/wiki/Text_segmentation#Word_segmentation  \n",
    "Letters by themselves aren't particularly meaningful, however full words by themselves are.  \n",
    "**Punctuation** should be considered its own token: it adds to the sentence meaning but not the individual word it is adjacent to.  \n",
    "Tokenizers are almost always constructed from a **deterministic white-box ruleset**.  \n",
    "Most commonly tokenizers use a complex **regex** expression to parse the text into words.\n",
    "\n",
    "\n",
    "There do exist NLP algorithms that don't tokenize sentences into distinct words: for example byte-pair encoding.\n",
    "https://en.wikipedia.org/wiki/Byte_pair_encoding  \n",
    "There also exist phonetic tokenizers, which can be good at capturing slang, and acronyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"If you knew how nuanced this gets, you wouldn't believe it!\"\n",
    "#doc = \"Why use many words, when fewer words do the trick?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK has a wide variety of tokenizers\n",
    "# https://www.nltk.org/api/nltk.tokenize.html\n",
    "nltk_space_tokenizer = nltk.tokenize.SpaceTokenizer()\n",
    "nltk_space_tokenized_doc = nltk_space_tokenizer.tokenize(doc)\n",
    "print(nltk_space_tokenized_doc)\n",
    "\n",
    "nltk_treebank_tokenizer = nltk.tokenize.treebank.TreebankWordTokenizer()\n",
    "nltk_treebank_tokenized_doc = nltk_treebank_tokenizer.tokenize(doc)\n",
    "print(nltk_treebank_tokenized_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SpaCy automatically tokenizes your document as part of its standard procedure\n",
    "spacy_doc = spacy_parser(doc)\n",
    "spacy_tokenized_doc = list(map(str, spacy_doc))\n",
    "print(spacy_tokenized_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization and Stemming\n",
    "https://en.wikipedia.org/wiki/Morphology_(linguistics)  \n",
    "https://en.wikipedia.org/wiki/Lemma_(morphology)  \n",
    "https://en.wikipedia.org/wiki/Stemming  \n",
    "https://en.wikipedia.org/wiki/Lemmatisation  \n",
    "Often the exact **morphology** of a word isn't very important to the meaning of a sentence. We just want the root of the word to capture its meaning.  \n",
    "**Stemming** is a method of obtaining the root of a word by simply matching the suffix of a word and removing it.  \n",
    "**Lemmatization** is a method of obtaining the root of a word by looking up the word in a dictionary that maps to a root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_snowball_stemmer = nltk.stem.snowball.EnglishStemmer()\n",
    "nltk_stemmed_doc = list(map(nltk_snowball_stemmer.stem, nltk_treebank_tokenized_doc))\n",
    "print(nltk_stemmed_doc)\n",
    "\n",
    "nltk_wordnet_lemmatizer = nltk.stem.WordNetLemmatizer() \n",
    "nltk_lemmatized_doc = list(map(nltk_wordnet_lemmatizer.lemmatize, nltk_treebank_tokenized_doc))\n",
    "print(nltk_lemmatized_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_lemmatized_doc = list(map(lambda token: token.lemma_, spacy_doc))\n",
    "print(spacy_lemmatized_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words\n",
    "https://en.wikipedia.org/wiki/Stop_words  \n",
    "Some words are considered insignificant to the meaning of the document. The solution is simply to filter those tokens out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "nltk_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "print(nltk_stopwords)\n",
    "print('')\n",
    "nltk_stopword_doc = list(filter(lambda token: token not in nltk_stopwords, nltk_lemmatized_doc))\n",
    "print(nltk_stopword_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_stopwords = spacy_parser.Defaults.stop_words\n",
    "print(spacy_stopwords)\n",
    "print('')\n",
    "spacy_doc_stopword_doc = list(map(str,filter(lambda token: not token.is_stop, spacy_doc)))\n",
    "print(spacy_doc_stopword_doc)\n",
    "print('')\n",
    "spacy_doc_stopword_doc = list(map(str,filter(lambda token: not token in spacy_stopwords, spacy_lemmatized_doc)))\n",
    "print(spacy_doc_stopword_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling Correction and Edit Distance\n",
    "https://en.wikipedia.org/wiki/Spell_checker  \n",
    "https://en.wikipedia.org/wiki/Edit_distance  \n",
    "https://en.wikipedia.org/wiki/Levenshtein_distance  \n",
    "Often raw data will come with typoes, slang, jargon, and acronyms/initialisms, and other features of real-life writing in practice.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "https://en.wikipedia.org/wiki/Bag-of-words_model  \n",
    "https://en.wikipedia.org/wiki/Vector_space_model  \n",
    "A Bag Of Words (BOW) model assumes the order of words isn't important to a sentence's meaning: it's simply what words are present and how many there are.  \n",
    "The motivation behind BOW models is to easily vectorize documents, with the dimensions corresponging to different tokens. Note how orthogonality can be thought of as representing the 'difference in meaning' of different tokens. Also note how idenpendence between the vector dimensions means context is lost in this type of encoding.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['The man is a man', \n",
    "          'The woman is the woman']\n",
    "tokenized_corpus = map(lambda doc: doc.lower().split(), corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding\n",
    "https://en.wikipedia.org/wiki/One-hot  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html  \n",
    "One-Hot Encoding is a technique for vectorizing discrete finite sets.  \n",
    "Each possible element of the universal set corresponds to a dimension of the vector.  \n",
    "Thus, a given set can be encoded as a vector by placing a 1 in each dimension for which its corresponding element apprears in the set.\n",
    "\n",
    "Intuitively, each dimesion of the vector is a boolean value corresponding to \"does this element appear in the set?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OneHotEncoder = sklearn.preprocessing.MultiLabelBinarizer()\n",
    "OneHotCorpus = OneHotEncoder.fit_transform(tokenized_corpus)\n",
    "print(OneHotEncoder.classes_)\n",
    "print(OneHotCorpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html  \n",
    "Also known as \"word count\" vectorizes documents by mapping every word in the corpus to the number of times it appears in a document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountEncoder = sklearn.feature_extraction.text.CountVectorizer(token_pattern=r'[a-z]+')\n",
    "CountCorpus = CountEncoder.fit_transform(corpus)\n",
    "print(CountEncoder.get_feature_names())\n",
    "print(CountCorpus.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "https://en.wikipedia.org/wiki/Tf%E2%80%93idf  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html  \n",
    "TF-IDF: Term Frequencyâ€“Inverse Document Frequency  \n",
    "TF-IDF uses both the counts of a word in its document, as well as its prevelence in the corpus to determine its relevance.  \n",
    "Thus terms that appear very frequenty everywhere and don't provide much information are weighted lower, where as words that appear in very few documents, but quite frequently within a document will be weighted highly.  \n",
    "\n",
    "The formula for the TF-IDF value, $\\mathrm{tfidf}$, of a term $t$ in a document $d$ is given by  \n",
    "$$ \\mathrm{tfidf}_{t, d} = \\frac{\\mathrm{f}_{t, d}}{\\sum_{t' \\in T} \\mathrm{f}_{t', d}} \\log \\left ( \\frac{\\left | D \\right |}{\\left | \\left \\{ d' \\in D \\mid \\mathrm{f}_{t, d'} > 0 \\right \\} \\right |} \\right )$$  \n",
    "where:  \n",
    "$\\mathrm{tfidf}_{t, d}$ is the TF-IDF value of term $t$ in document $d$  \n",
    "$\\mathrm{f}_{t, d}$ is the number of times term $t$ appears in document $d$  \n",
    "$T$ is the universal set of terms  \n",
    "$D$ is the universal set of documents  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDFEncoder = sklearn.feature_extraction.text.TfidfVectorizer(token_pattern=r'[a-z]+')\n",
    "TFIDFCorpus = TFIDFEncoder.fit_transform(corpus)\n",
    "print(TFIDFEncoder.get_feature_names())\n",
    "print(TFIDFCorpus.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-grams\n",
    "https://en.wikipedia.org/wiki/N-gram  \n",
    "n-grams are groups of tokens of cardinality n that appear contiguously in their documents.  \n",
    "A small amount of local context can be added to the bag-of-words model by extending tokens into n-grams.  \n",
    "For example, marking the existance of the bigram 'not good' in a document would give more information about its content than the tokens 'not' and 'good' sparately: this small amount of local context can have significant meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGramCountEncoder = sklearn.feature_extraction.text.CountVectorizer(token_pattern=r'[a-z]+', ngram_range=(1,2))\n",
    "NGramCountCorpus = NGramCountEncoder.fit_transform(corpus)\n",
    "print(NGramCountEncoder.get_feature_names())\n",
    "print(NGramCountCorpus.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Models\n",
    "\n",
    "### Word2Vec\n",
    "\n",
    "### BERT ELMo\n",
    "\n",
    "### Syntax Trees and Grammars\n",
    "\n",
    "## Non-Vector models\n",
    "\n",
    "### Princeton Wordnet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
